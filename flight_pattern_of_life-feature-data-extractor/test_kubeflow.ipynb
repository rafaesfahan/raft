{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install lightning\n",
    "!pip3 install torch torchvision torchaudio\n",
    "\n",
    "!pip install tensorboard\n",
    "!python -m pip install lightning\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install folium\n",
    "!pip install selenium\n",
    "!pip install hydra-core pytorch-lightning omegaconf torch torchvision torchaudio folium scipy matplotlib numpy pandas selenium\n",
    "!pip install mlflow\n",
    "!pip install kfp[kubernetes]\n",
    "!pip install kserve\n",
    "!pip install minio\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_minio_stuff():\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    import boto3\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # # AWS access / secret keys are on the env file\n",
    "    # minio_endpoint = os.getenv('AWS_ENDPOINT_URL_S3')\n",
    "\n",
    "    # print(minio_endpoint)\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    import os\n",
    "\n",
    "    # Hardcoded AWS access and secret keys\n",
    "    aws_access_key_id = 'minio'\n",
    "    aws_secret_access_key = 'minio123'\n",
    "    aws_endpoint_url_s3 = 'http://minio-service.kubeflow:9000'\n",
    "\n",
    "    # Set the environment variables\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key_id\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_access_key\n",
    "    os.environ['AWS_ENDPOINT_URL_S3'] = aws_endpoint_url_s3\n",
    "\n",
    "    # Now you can access them as environment variables\n",
    "    minio_endpoint = os.getenv('AWS_ENDPOINT_URL_S3')\n",
    "    print(minio_endpoint)\n",
    "    \n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    return s3\n",
    "\n",
    "def download_dir(prefix, local, bucket, client):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - prefix: pattern to match in s3\n",
    "    - local: local path to folder in which to place files\n",
    "    - bucket: s3 bucket with target contents\n",
    "    - client: initialized s3 client object\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    dirs = []\n",
    "    next_token = ''\n",
    "    base_kwargs = {\n",
    "        'Bucket':bucket,\n",
    "        'Prefix':prefix,\n",
    "    }\n",
    "    while next_token is not None:\n",
    "        kwargs = base_kwargs.copy()\n",
    "        if next_token != '':\n",
    "            kwargs.update({'ContinuationToken': next_token})\n",
    "        results = client.list_objects_v2(**kwargs)\n",
    "        contents = results.get('Contents')\n",
    "        for i in contents:\n",
    "            k = i.get('Key')\n",
    "            if k[-1] != '/':\n",
    "                keys.append(k)\n",
    "            else:\n",
    "                dirs.append(k)\n",
    "        next_token = results.get('NextContinuationToken')\n",
    "    for d in dirs:\n",
    "        dest_pathname = os.path.join(local, d)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "    for k in keys:\n",
    "        dest_pathname = os.path.join(local, k)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "        client.download_file(bucket, k, dest_pathname)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, InputPath, OutputPath\n",
    "from kfp import kubernetes\n",
    "\n",
    "# In Jupyter Notebook\n",
    "from main import main  # Import the main function from main.py\n",
    "\n",
    "EXPERIMENT_NUMBER = 27\n",
    "\n",
    "\n",
    "def train_model_op():\n",
    "    import subprocess\n",
    "    import os\n",
    "    import boto3\n",
    "\n",
    "\n",
    "    # from main import main\n",
    "    # from hydra import compose, initialize\n",
    "    # # Initialize Hydra and compose the configuration\n",
    "    # initialize(config_path='conf')\n",
    "    # cfg = compose(config_name='config')\n",
    "    # main(cfg)\n",
    "    \n",
    "    def setup_minio_stuff():\n",
    "        # # AWS access / secret keys are on the env file\n",
    "        # minio_endpoint = os.getenv('AWS_ENDPOINT_URL_S3')\n",
    "\n",
    "        # print(minio_endpoint)\n",
    "\n",
    "\n",
    "        ################################################\n",
    "\n",
    "        # Hardcoded AWS access and secret keys\n",
    "        aws_access_key_id = 'minio'\n",
    "        aws_secret_access_key = 'minio123'\n",
    "        aws_endpoint_url_s3 = 'http://minio-service.kubeflow:9000'\n",
    "\n",
    "        # Set the environment variables\n",
    "        os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key_id\n",
    "        os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_access_key\n",
    "        os.environ['AWS_ENDPOINT_URL_S3'] = aws_endpoint_url_s3\n",
    "\n",
    "        # Now you can access them as environment variables\n",
    "        minio_endpoint = os.getenv('AWS_ENDPOINT_URL_S3')\n",
    "        print(minio_endpoint)\n",
    "\n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "        return s3\n",
    "\n",
    "    def download_dir(prefix, local, bucket, client):\n",
    "        \"\"\"\n",
    "        params:\n",
    "        - prefix: pattern to match in s3\n",
    "        - local: local path to folder in which to place files\n",
    "        - bucket: s3 bucket with target contents\n",
    "        - client: initialized s3 client object\n",
    "        \"\"\"\n",
    "        keys = []\n",
    "        dirs = []\n",
    "        next_token = ''\n",
    "        base_kwargs = {\n",
    "            'Bucket':bucket,\n",
    "            'Prefix':prefix,\n",
    "        }\n",
    "        while next_token is not None:\n",
    "            kwargs = base_kwargs.copy()\n",
    "            if next_token != '':\n",
    "                kwargs.update({'ContinuationToken': next_token})\n",
    "            results = client.list_objects_v2(**kwargs)\n",
    "            contents = results.get('Contents')\n",
    "            for i in contents:\n",
    "                k = i.get('Key')\n",
    "                if k[-1] != '/':\n",
    "                    keys.append(k)\n",
    "                else:\n",
    "                    dirs.append(k)\n",
    "            next_token = results.get('NextContinuationToken')\n",
    "        for d in dirs:\n",
    "            dest_pathname = os.path.join(local, d)\n",
    "            if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "                os.makedirs(os.path.dirname(dest_pathname))\n",
    "        for k in keys:\n",
    "            dest_pathname = os.path.join(local, k)\n",
    "            if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "                os.makedirs(os.path.dirname(dest_pathname))\n",
    "            client.download_file(bucket, k, dest_pathname)\n",
    "\n",
    "    EXPERIMENT_STRING = 'experiment=experiment_fast_run_mil'\n",
    "    s3 = setup_minio_stuff()\n",
    "    download_dir('','/home/jovyan/dataset', 'dataset', s3)\n",
    "    \n",
    "    import time\n",
    "    # time.sleep(600)\n",
    "    \n",
    "    \n",
    "    subprocess.run(['python3', '/home/jovyan/flight_pattern_of_life/main.py', EXPERIMENT_STRING])\n",
    "    \n",
    "    # HYDRA_FULL_ERROR=1 /opt/anaconda3/envs/trajectory/bin/python /Users/aleksandranikevich/Desktop/AircraftTrajectory/REPO/flight_pattern_of_life/main.py experiment=experiment_fast_run_local\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f'PyTorch Lightning Pipeline {EXPERIMENT_NUMBER}',\n",
    "    description='A pipeline that trains a PyTorch Lightning model using Hydra'\n",
    ")\n",
    "def pytorch_lightning_pipeline():\n",
    "    import subprocess\n",
    "    \n",
    "    train_op = train_model_op().set_display_name('Train Model').set_caching_options(False)\n",
    "    train_op.set_env_variable('HYDRA_FULL_ERROR', '1')\n",
    "    # ###volume_name = \"aircraft-trajectory-volume\"\n",
    "    # volume_name = \"dataset\"\n",
    "    \n",
    "    # ###mount_path='/home/jovyan/aircraft-trajectory-volume'\n",
    "    # mount_path = '/home/jovyan/dataset/ADIZ/Flightpath/'\n",
    "    # kubernetes.mount_pvc(\n",
    "    #     train_op,\n",
    "    #     pvc_name=volume_name,\n",
    "    #     mount_path=mount_path,    #'/data',\n",
    "    # )\n",
    "    \n",
    "    \n",
    "    # # mount_dir = \"/mnt\"\n",
    "    \n",
    "    # subprocess.run(['ls', '-l', mount_path])\n",
    "    # #task = create_step_prepare_data().add_pvolumes({data_path: dsl.PipelineVolume(pvc=volume_name)})\n",
    "    \n",
    "    \n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_file = \"pytorch-lightning-pipeline.yaml\"\n",
    "with open(pipeline_file, \"w\") as f:\n",
    "    kfp.compiler.Compiler().compile(\n",
    "        pipeline_func=pytorch_lightning_pipeline,\n",
    "        package_path=f.name\n",
    "    )\n",
    "\n",
    "# Experiment setup and run\n",
    "EXPERIMENT_NAME = 'AA test pipeline'\n",
    "from kfp.client.set_volume_credentials import ServiceAccountTokenVolumeCredentials\n",
    "\n",
    "namespace = \"kubeflow\"\n",
    "credentials = ServiceAccountTokenVolumeCredentials(path='/var/run/secrets/kubeflow/pipelines/token')\n",
    "client = kfp.Client(host=f\"http://ml-pipeline-ui.{namespace}\", credentials=credentials)\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pytorch_lightning_pipeline,\n",
    "    arguments={},\n",
    "    experiment_name=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "pipeline = client.upload_pipeline(\n",
    "    pipeline_package_path=pipeline_file,\n",
    "    pipeline_name=f\"PytorchLightningPipeline {EXPERIMENT_NUMBER}\",\n",
    "    description=f\"Test pipeline pytorch lightning {EXPERIMENT_NUMBER}\"\n",
    ")\n",
    "\n",
    "# pipeline_name = \"PytorchLightningPipeline 35\"\n",
    "# pipeline = client.upload_pipeline_version(\n",
    "#     pipeline_package_path=pipeline_file,\n",
    "#     pipeline_name=pipeline_name,\n",
    "#     pipeline_version_name=\"0.0.1\",\n",
    "#     description=\"some pipeline\"\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kfp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dsl\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m component, InputPath, OutputPath\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kfp'"
     ]
    }
   ],
   "source": [
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, InputPath, OutputPath\n",
    "from kfp import kubernetes\n",
    "\n",
    "# In Jupyter Notebook\n",
    "from main import main  # Import the main function from main.py\n",
    "\n",
    "EXPERIMENT_NUMBER = 1\n",
    "EXPERIMENT_STRING = 'experiment=experiment_fast_run_mil'\n",
    "\n",
    "\n",
    "\n",
    "@component(\n",
    "    #base_image=\"ghcr.io/raft-tech/raft-kubeflow:yolo-train-v1\",\n",
    "    base_image=\"ghcr.io/raft-tech/aircraft_trajectory:0.1.0\",\n",
    "    ##packages_to_install=['mlflow', 'pytorch-lightning']\n",
    ")\n",
    "def train_model_op():\n",
    "    import subprocess\n",
    "\n",
    "\n",
    "    # from main import main\n",
    "    # from hydra import compose, initialize\n",
    "    # # Initialize Hydra and compose the configuration\n",
    "    # initialize(config_path='conf')\n",
    "    # cfg = compose(config_name='config')\n",
    "    # main(cfg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    subprocess.run(['python3', '/home/jovyan/flight_pattern_of_life/main.py', EXPERIMENT_STRING])\n",
    "    \n",
    "    # HYDRA_FULL_ERROR=1 /opt/anaconda3/envs/trajectory/bin/python /Users/aleksandranikevich/Desktop/AircraftTrajectory/REPO/flight_pattern_of_life/main.py experiment=experiment_fast_run_local\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f'PyTorch Lightning Pipeline {EXPERIMENT_NUMBER}',\n",
    "    description='A pipeline that trains a PyTorch Lightning model using Hydra'\n",
    ")\n",
    "def pytorch_lightning_pipeline():\n",
    "    import subprocess\n",
    "    \n",
    "    train_op = train_model_op().set_display_name('Train Model').set_caching_options(False)\n",
    "    train_op.set_env_variable('HYDRA_FULL_ERROR', '1')\n",
    "    volume_name = \"aircraft-trajectory-volume\"\n",
    "    ######train_op.add_pvolumes({'data_path': PipelineVolume(pvc=volume_name)})\n",
    "    \n",
    "    mount_path='/home/jovyan/aircraft-trajectory-volume'\n",
    "    kubernetes.mount_pvc(\n",
    "        train_op,\n",
    "        pvc_name=volume_name,\n",
    "        mount_path=mount_path,    #'/data',\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # mount_dir = \"/mnt\"\n",
    "    \n",
    "    subprocess.run(['ls', '-l', mount_path])\n",
    "    #task = create_step_prepare_data().add_pvolumes({data_path: dsl.PipelineVolume(pvc=volume_name)})\n",
    "    \n",
    "    \n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_file = \"pytorch-lightning-pipeline.yaml\"\n",
    "with open(pipeline_file, \"w\") as f:\n",
    "    kfp.compiler.Compiler().compile(\n",
    "        pipeline_func=pytorch_lightning_pipeline,\n",
    "        package_path=f.name\n",
    "    )\n",
    "\n",
    "# Experiment setup and run\n",
    "EXPERIMENT_NAME = 'AA test pipeline'\n",
    "from kfp.client.set_volume_credentials import ServiceAccountTokenVolumeCredentials\n",
    "\n",
    "namespace = \"kubeflow\"\n",
    "credentials = ServiceAccountTokenVolumeCredentials(path='/var/run/secrets/kubeflow/pipelines/token')\n",
    "client = kfp.Client(host=f\"http://ml-pipeline-ui.{namespace}\", credentials=credentials)\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pytorch_lightning_pipeline,\n",
    "    arguments={},\n",
    "    experiment_name=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "pipeline = client.upload_pipeline(\n",
    "    pipeline_package_path=pipeline_file,\n",
    "    pipeline_name=f\"PytorchLightningPipeline {EXPERIMENT_NUMBER}\",\n",
    "    description=f\"Test pipeline pytorch lightning {EXPERIMENT_NUMBER}\"\n",
    ")\n",
    "\n",
    "# pipeline_name = \"PytorchLightningPipeline 35\"\n",
    "# pipeline = client.upload_pipeline_version(\n",
    "#     pipeline_package_path=pipeline_file,\n",
    "#     pipeline_name=pipeline_name,\n",
    "#     pipeline_version_name=\"0.0.1\",\n",
    "#     description=\"some pipeline\"\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import kserve\n",
    "import requests\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "\n",
    "\n",
    "class AircraftTrajectoryPredictor(kserve.Model):\n",
    "    def __init__(self, name: str, model_file: str):\n",
    "        super().__init__(name)\n",
    "        self.name = name\n",
    "        self.model_file = model_file\n",
    "        self.model = None\n",
    "        self.ready = False\n",
    "        self.minio_client = Minio(\n",
    "            os.environ.get(\"MINIO_ENDPOINT\", \"minio-service.kubeflow:9000\"),\n",
    "            access_key=\"minio\", #os.environ.get(\"MINIO_ACCESS_KEY\"),\n",
    "            secret_key=\"minio123\", #os.environ.get(\"MINIO_SECRET_KEY\"),\n",
    "            secure=False,  # Set to True if using HTTPS\n",
    "        )\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            # First, check if the model file exists locally\n",
    "            if os.path.exists(self.model_file):\n",
    "                model_path = self.model_file\n",
    "            else:\n",
    "                # If not local, check if the model file exists in the MinIO bucket\n",
    "                bucket_name = os.environ.get(\"MINIO_BUCKET\", \"aanikevich\")\n",
    "                self.minio_client.stat_object(bucket_name, self.model_file)\n",
    "\n",
    "                # If the file exists in MinIO, download it to a temporary location\n",
    "                model_path = f\"/tmp/{self.model_file}\"\n",
    "                self.minio_client.fget_object(bucket_name, self.model_file, model_path)\n",
    "\n",
    "            # Load the model using the file path\n",
    "                self.model = torch.jit.load(model_path)\n",
    "            # Clean up the temporary file if it was downloaded from MinIO\n",
    "            if model_path.startswith(\"/tmp/\"):\n",
    "                os.remove(model_path)\n",
    "            \n",
    "            # Model ready\n",
    "            self.ready = True\n",
    "        except S3Error as e:\n",
    "            raise RuntimeError(f\"Error accessing model file in MinIO: {str(e)}\")\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(f\"Model file not found: {self.model_file}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "    def predict(self, request: Dict, headers: Dict) -> Dict:\n",
    "        # Implement prediction logic here\n",
    "        if not self.ready:\n",
    "            raise RuntimeError(\"Model is not loaded\")\n",
    "\n",
    "        # Process input and make predictions\n",
    "        input_data = request[\"instances\"]\n",
    "        # Raw input data\n",
    "        print(input_data)\n",
    "\n",
    "        out_loaded = self.model(input_data)\n",
    "        \n",
    "        results = out_loaded.detach().cpu().numpy().tolist()\n",
    "\n",
    "        # Return the JSON response\n",
    "        return {\"predictions\": results}\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AircraftTrajectoryPredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels_jit2/scripted_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maircraft-trajectory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m predictor \u001b[38;5;241m=\u001b[39m AircraftTrajectoryPredictor(model_name, model_file)\n\u001b[1;32m     12\u001b[0m predictor\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Start the model server without running into the nested event loop issue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AircraftTrajectoryPredictor' is not defined"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import kserve\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "model_file = \"models_jit2/scripted_model.pt\"\n",
    "model_name = \"aircraft-trajectory\"\n",
    "predictor = AircraftTrajectoryPredictor(model_name, model_file)\n",
    "predictor.load()\n",
    "\n",
    "# Start the model server without running into the nested event loop issue\n",
    "kserve.ModelServer().start([predictor]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m example_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_input}\n\u001b[1;32m      4\u001b[0m predictor\u001b[38;5;241m.\u001b[39mpredict(example_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "example_input = torch.rand(3, 5, 100)\n",
    "example_dict = {\"instances\": example_input}\n",
    "\n",
    "predictor.predict(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajectory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
