{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import loggers\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy as np\n",
    "\n",
    "from net import SimpleNet\n",
    "from datamodule import Datamodule\n",
    "from model import FlightModel\n",
    "\n",
    "from file_parsing_utils import create_csv_dict\n",
    "from coordinate_transform import CoordinateEnum, helper_get_coordinate_system_based_on_enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n",
      "debug use batch norm:  False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     22\u001b[0m net \u001b[38;5;241m=\u001b[39m SimpleNet(in_channels\u001b[38;5;241m=\u001b[39min_channels, \n\u001b[1;32m     23\u001b[0m                 out_channels\u001b[38;5;241m=\u001b[39mout_channels, \n\u001b[1;32m     24\u001b[0m                 intermediate_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     31\u001b[0m                 bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m individual_flights_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/aleksandranikevich/Desktop/AircraftTrajectory/data/Individual_Flights/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m flight_dfs \u001b[38;5;241m=\u001b[39m create_csv_dict(individual_flights_dir)\n\u001b[1;32m     38\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m Datamodule(all_flight_dataframes_dict \u001b[38;5;241m=\u001b[39m flight_dfs, \n\u001b[1;32m     39\u001b[0m                         num_input_rows_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, \n\u001b[1;32m     40\u001b[0m                         min_rows_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m                         num_workers \u001b[38;5;241m=\u001b[39m num_workers, \n\u001b[1;32m     48\u001b[0m                         pin_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m     50\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n",
      "File \u001b[0;32m~/Desktop/AircraftTrajectory/REPO/flight_pattern_of_life/file_parsing_utils.py:24\u001b[0m, in \u001b[0;36mcreate_csv_dict\u001b[0;34m(dir_individual_flights)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m dict_ids \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 24\u001b[0m id_keys \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(flight_folder_dir) \n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_key \u001b[38;5;129;01min\u001b[39;00m id_keys:\n\u001b[1;32m     26\u001b[0m     id_key_dir \u001b[38;5;241m=\u001b[39m flight_folder_dir \u001b[38;5;241m+\u001b[39m id_key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_workers = 8\n",
    "\n",
    "max_num_val_maps = 8\n",
    "coordinate_system_enum = CoordinateEnum.LatLongCoordinates\n",
    "coordinate_system = helper_get_coordinate_system_based_on_enum(coordinate_enum=coordinate_system_enum)\n",
    "\n",
    "auxiliary_input_channels = [\n",
    "                            \"diff_time\", \n",
    "                            \"flight_course_corrected\", \n",
    "                            \"flight_course_unknown\", \n",
    "                            ]\n",
    "\n",
    "auxiliary_output_channels = []\n",
    "\n",
    "\n",
    "\n",
    "num_res_blocks = 8\n",
    "in_channels = len(coordinate_system) + len(auxiliary_input_channels)\n",
    "out_channels = len(coordinate_system)\n",
    "###dilation = np.concatenate([np.ones(1), np.random.randint(1, 5, num_res_blocks), np.ones(1)])\n",
    "dilation = [1 if i % 2 == 0 else 2 for i in range(num_res_blocks + 2)]\n",
    "net = SimpleNet(in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                intermediate_channels=128, \n",
    "                num_res_blocks=num_res_blocks, \n",
    "                num_output_rows=1, \n",
    "                dilation = dilation, \n",
    "                kernel_size = 27, \n",
    "                use_bn_norm=False, #True, \n",
    "                stride=1, \n",
    "                bias=True)\n",
    "\n",
    "\n",
    "individual_flights_dir = \"/Users/aleksandranikevich/Desktop/AircraftTrajectory/data/Individual_Flights/\"\n",
    "flight_dfs = create_csv_dict(individual_flights_dir)\n",
    "\n",
    "\n",
    "datamodule = Datamodule(all_flight_dataframes_dict = flight_dfs, \n",
    "                        num_input_rows_total = 1000, \n",
    "                        min_rows_input = 100, \n",
    "                        num_output_rows = 1, \n",
    "                        coordinate_system_enum = coordinate_system_enum,\n",
    "                        auxiliary_input_channels = auxiliary_input_channels,\n",
    "                        auxiliary_output_channels = auxiliary_output_channels,\n",
    "                        train_prop = 0.8, \n",
    "                        batch_size = 32, \n",
    "                        num_workers = num_workers, \n",
    "                        pin_memory = True,)\n",
    "\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "\n",
    "# dummy tensor\n",
    "test_dataloader_iterator = test_dataloader.__iter__()\n",
    "tensors_dict = next(test_dataloader_iterator)\n",
    "dummy_input_tensor = tensors_dict[\"input_tensor\"]\n",
    "dummy_output = net(dummy_input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28.3660,  28.3660,  28.3660,  28.3660,  28.3660,  28.3660,  28.3660,\n",
       "          28.3660,  28.3660,  28.3660,  28.3660,  28.3660,  28.3660,  28.3660,\n",
       "          28.3660,  28.3660,  28.3660,  28.3660,  28.3660,  28.3660],\n",
       "        [-90.2972, -90.2972, -90.2972, -90.2972, -90.2972, -90.2972, -90.2972,\n",
       "         -90.2972, -90.2972, -90.2972, -90.2972, -90.2972, -90.2972, -90.2972,\n",
       "         -90.2972, -90.2972, -90.2972, -90.2972, -90.2972, -90.2972]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors_dict[\"input_tensor\"][0, :2, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tensors_dict\n",
    "\n",
    "batch = normalize_center(batch, coordinate_system_enum=coordinate_system_enum)\n",
    "input_tensor = batch[\"input_tensor\"]\n",
    "output_tensor = batch[\"output_tensor\"]\n",
    "list_full_flightpaths = batch[\"meta_flightpath\"]\n",
    "list_of_msn = batch[\"meta_msn\"]\n",
    "pred_tensor = net(input_tensor) # torch.Size([32, 4, 1])\n",
    "b = input_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 29.1860,  29.1899,  29.1935,  29.1970,  29.2007,  29.2041,  29.2073,\n",
       "          29.2106,  29.2140,  29.2174],\n",
       "        [-90.5450, -90.5462, -90.5472, -90.5483, -90.5493, -90.5504, -90.5514,\n",
       "         -90.5523, -90.5534, -90.5544]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = batch['normalization_tensor'][0].clone()\n",
    "orig = input_tensor[0, :2, -10:] + n\n",
    "orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8514, -0.8514, -0.8514, -0.8514, -0.8514, -0.8514, -0.8514, -0.8514,\n",
       "         -0.8514, -0.8514, -0.8514, -0.8514, -0.8514, -0.8514, -0.8514, -0.8514,\n",
       "         -0.8514, -0.8514, -0.8514, -0.8514],\n",
       "        [ 0.2572,  0.2572,  0.2572,  0.2572,  0.2572,  0.2572,  0.2572,  0.2572,\n",
       "          0.2572,  0.2572,  0.2572,  0.2572,  0.2572,  0.2572,  0.2572,  0.2572,\n",
       "          0.2572,  0.2572,  0.2572,  0.2572]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_tensor\"][0, :2, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def iterative_path_predict_step(input_tensor, prediction_tensor, coordinate_system_enum, len_coordinate_system):\n",
    "#     with torch.no_grad():\n",
    "#         _, _, num_input_rows = input_tensor.shape\n",
    "#         _, _, num_output_rows = prediction_tensor.shape\n",
    "#         input_tensor_final_n_rows = input_tensor[:, :, -num_output_rows:]  \n",
    "#         stacked_orig_pred = torch.concat([input_tensor, input_tensor_final_n_rows], dim=-1) # keep auxilary info, pred tensor will then replace the coordinate values\n",
    "#         stacked_orig_pred[:, :len_coordinate_system, -num_output_rows:] = prediction_tensor \n",
    "#         new_normalization_tensor = stacked_orig_pred[:, :len_coordinate_system, -1: ]\n",
    "\n",
    "#         print(\"inside step: \", stacked_orig_pred[0, 0, -10:])\n",
    "#         print(\"inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "#         stacked_orig_pred[:, :len_coordinate_system, :] = stacked_orig_pred[:, :len_coordinate_system, :] - new_normalization_tensor # new input tensor\n",
    "\n",
    "#         print(\"Before end of WITH inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "#     print(\"BEFORE RETURN inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "#     return torch.clone(stacked_orig_pred[:, :, -num_input_rows:]), torch.clone(new_normalization_tensor)\n",
    "\n",
    "def iterative_path_predict_step(input_tensor, prediction_tensor, coordinate_system_enum, len_coordinate_system):\n",
    "    with torch.no_grad():\n",
    "        _, _, num_input_rows = input_tensor.shape\n",
    "        _, _, num_output_rows = prediction_tensor.shape\n",
    "        input_tensor_final_n_rows = input_tensor[:, :, -num_output_rows:]  \n",
    "        stacked_orig_pred = torch.concat([input_tensor, input_tensor_final_n_rows], dim=-1) # keep auxiliary info, pred tensor will then replace the coordinate values\n",
    "        stacked_orig_pred[:, :len_coordinate_system, -num_output_rows:] = prediction_tensor \n",
    "        new_normalization_tensor = stacked_orig_pred[:, :len_coordinate_system, -1:].clone()  # Clone to avoid modifying the original tensor\n",
    "\n",
    "        print(\"inside step: \", stacked_orig_pred[0, 0, -10:])\n",
    "        print(\"inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "        stacked_orig_pred[:, :len_coordinate_system, :] = stacked_orig_pred[:, :len_coordinate_system, :] - new_normalization_tensor # new input tensor\n",
    "\n",
    "        print(\"Before end of WITH inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "    print(\"BEFORE RETURN inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "    return torch.clone(stacked_orig_pred[:, :, -num_input_rows:]), torch.clone(new_normalization_tensor)\n",
    "\n",
    "\n",
    "\n",
    "def iterative_path_predict(batch_dict, model, coordinate_system_enum, num_predict_steps, bool_normalize_center = False):\n",
    "    \"\"\"\n",
    "    iteratively predict N steps using the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coordinate_system = helper_get_coordinate_system_based_on_enum(coordinate_system_enum)\n",
    "    len_coordinate_system = len(coordinate_system )\n",
    "\n",
    "    if bool_normalize_center:\n",
    "        batch_dict = normalize_center(batch_dict, coordinate_system_enum=coordinate_system_enum)\n",
    "\n",
    "    input_tensor = batch_dict[\"input_tensor\"]\n",
    "    normalization_tensor_cumulative = torch.clone(batch_dict[\"normalization_tensor\"])\n",
    "    print(\"at the beginning normalization_tensor_cumulative: \", normalization_tensor_cumulative[0, 0])\n",
    "    # prediction_tensor = model(input_tensor)\n",
    "    # print(\"prediction_tensor   : \", prediction_tensor[0, 0, :10])\n",
    "\n",
    "    pred_tensors_list = []\n",
    "\n",
    "    for i in range(num_predict_steps):\n",
    "        prediction_tensor = model(input_tensor)\n",
    "        input_tensor, normalization_tensor = iterative_path_predict_step(input_tensor.clone(), prediction_tensor.clone(), coordinate_system_enum, len_coordinate_system)\n",
    "        print(\"prediction_tensor: \", prediction_tensor[0, 0, :10])\n",
    "        print(\"normalization_tensor: \", normalization_tensor[0, 0])\n",
    "        print(\"new input_tensor: \", input_tensor[0, 0, -10:])\n",
    "        #######normalization_tensor_cumulative = normalization_tensor_cumulative + normalization_tensor.clone()\n",
    "        pred_tensors_list.append(prediction_tensor.clone())\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    \n",
    "    iterative_predictions_tensor = torch.concat(pred_tensors_list, dim=-1) # with normalization: normalization_tensor_copy\n",
    "    print(\"before CUMSUM PRED TENSOR: \", iterative_predictions_tensor[0])\n",
    "    iterative_predictions_tensor = torch.cumsum(iterative_predictions_tensor, dim=-1) #np.cumsum(iterative_predictions_tensor, axis=-1)\n",
    "    print(\"at the end iterative_predictions_tensor: \", iterative_predictions_tensor[0])\n",
    "    print(\"at the end normalization_tensor_cumulative: \", normalization_tensor_cumulative[0, 0])\n",
    "    iterative_predictions_tensor = un_normalize_center_predictions(iterative_predictions_tensor, normalization_tensor_cumulative) \n",
    "    iterative_predictions_tensor_np = iterative_predictions_tensor.detach().cpu().numpy()\n",
    "    return iterative_predictions_tensor_np\n",
    "\n",
    "\n",
    "# def iterative_path_predict(batch_dict, model, coordinate_system_enum, num_predict_steps, bool_normalize_center = False):\n",
    "#     \"\"\"\n",
    "#     iteratively predict N steps using the model\n",
    "#     \"\"\"\n",
    "    \n",
    "#     coordinate_system = helper_get_coordinate_system_based_on_enum(coordinate_system_enum)\n",
    "#     len_coordinate_system = len(coordinate_system )\n",
    "\n",
    "#     if bool_normalize_center:\n",
    "#         batch_dict = normalize_center(batch_dict, coordinate_system_enum=coordinate_system_enum)\n",
    "\n",
    "#     input_tensor = batch_dict[\"input_tensor\"]\n",
    "#     normalization_tensor_cumulative = torch.clone(batch_dict[\"normalization_tensor\"])\n",
    "#     print(\"at the beginning normalization_tensor_cumulative: \", normalization_tensor_cumulative[0, 0])\n",
    "#     prediction_tensor = model(input_tensor)\n",
    "#     print(\"prediction_tensor   : \", prediction_tensor[0, 0, :10])\n",
    "\n",
    "#     pred_tensors_list = [torch.clone(prediction_tensor)] \n",
    "\n",
    "#     for i in range(num_predict_steps):\n",
    "#         input_tensor, normalization_tensor = iterative_path_predict_step(input_tensor, prediction_tensor, coordinate_system_enum, len_coordinate_system)\n",
    "#         prediction_tensor = model(input_tensor)\n",
    "#         print(\"prediction_tensor: \", prediction_tensor[0, 0, :10])\n",
    "#         print(\"normalization_tensor: \", normalization_tensor[0, 0])\n",
    "#         print(\"new input_tensor: \", input_tensor[0, 0, -10:])\n",
    "#         normalization_tensor_cumulative = normalization_tensor_cumulative + normalization_tensor\n",
    "#         pred_tensors_list.append(prediction_tensor)\n",
    "\n",
    "#         print(\"\")\n",
    "\n",
    "    \n",
    "#     iterative_predictions_tensor = torch.concat(pred_tensors_list, dim=-1) # with normalization: normalization_tensor_copy\n",
    "#     print(\"before CUMSUM PRED TENSOR: \", iterative_predictions_tensor[0])\n",
    "#     iterative_predictions_tensor = torch.cumsum(iterative_predictions_tensor, dim=-1) #np.cumsum(iterative_predictions_tensor, axis=-1)\n",
    "#     print(\"at the end iterative_predictions_tensor: \", iterative_predictions_tensor[0])\n",
    "#     print(\"at the end normalization_tensor_cumulative: \", normalization_tensor_cumulative[0, 0])\n",
    "#     iterative_predictions_tensor = un_normalize_center_predictions(iterative_predictions_tensor, normalization_tensor_cumulative) \n",
    "#     iterative_predictions_tensor_np = iterative_predictions_tensor.detach().cpu().numpy()\n",
    "#     return iterative_predictions_tensor_np\n",
    "\n",
    "\n",
    "\n",
    "def dummy_net(dummy_arg):\n",
    "    return 0.01 * torch.ones((32, 2, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dummy_net2(dummy_arg):\n",
    "    # Create a tensor with the desired values for timesteps\n",
    "    timestep_values = torch.tensor([0.01, 0.02, 0.03])\n",
    "    \n",
    "    # Repeat the tensor for each batch and channel\n",
    "    dummy_tensor = timestep_values.repeat(32, 2, 1)\n",
    "    \n",
    "    return dummy_tensor\n",
    "\n",
    "# # Example usage\n",
    "# dummy_arg = None  # Replace with actual argument if needed\n",
    "# output = dummy_net2(dummy_arg)\n",
    "# print(output.shape)\n",
    "# print(output[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the beginning normalization_tensor_cumulative:  tensor([29.2174])\n",
      "inside step:  tensor([-0.0204, -0.0168, -0.0134, -0.0101, -0.0068, -0.0034,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0504, -0.0468, -0.0434, -0.0401, -0.0368, -0.0334, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0401, -0.0368, -0.0334, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0701, -0.0668, -0.0634, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "inside step:  tensor([-0.0600, -0.0500, -0.0400, -0.0300, -0.0200, -0.0100,  0.0000,  0.0100,\n",
      "         0.0200,  0.0300])\n",
      "inside step new_normalization_tensor:  tensor([0.0300])\n",
      "Before end of WITH inside step new_normalization_tensor:  tensor([0.0300])\n",
      "BEFORE RETURN inside step new_normalization_tensor:  tensor([0.0300])\n",
      "prediction_tensor:  tensor([0.0100, 0.0200, 0.0300])\n",
      "normalization_tensor:  tensor([0.0300])\n",
      "new input_tensor:  tensor([-0.0900, -0.0800, -0.0700, -0.0600, -0.0500, -0.0400, -0.0300, -0.0200,\n",
      "        -0.0100,  0.0000])\n",
      "\n",
      "before CUMSUM PRED TENSOR:  tensor([[0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300,\n",
      "         0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300,\n",
      "         0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300,\n",
      "         0.0100, 0.0200, 0.0300],\n",
      "        [0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300,\n",
      "         0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300,\n",
      "         0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300, 0.0100, 0.0200, 0.0300,\n",
      "         0.0100, 0.0200, 0.0300]])\n",
      "at the end iterative_predictions_tensor:  tensor([[0.0100, 0.0300, 0.0600, 0.0700, 0.0900, 0.1200, 0.1300, 0.1500, 0.1800,\n",
      "         0.1900, 0.2100, 0.2400, 0.2500, 0.2700, 0.3000, 0.3100, 0.3300, 0.3600,\n",
      "         0.3700, 0.3900, 0.4200, 0.4300, 0.4500, 0.4800, 0.4900, 0.5100, 0.5400,\n",
      "         0.5500, 0.5700, 0.6000],\n",
      "        [0.0100, 0.0300, 0.0600, 0.0700, 0.0900, 0.1200, 0.1300, 0.1500, 0.1800,\n",
      "         0.1900, 0.2100, 0.2400, 0.2500, 0.2700, 0.3000, 0.3100, 0.3300, 0.3600,\n",
      "         0.3700, 0.3900, 0.4200, 0.4300, 0.4500, 0.4800, 0.4900, 0.5100, 0.5400,\n",
      "         0.5500, 0.5700, 0.6000]])\n",
      "at the end normalization_tensor_cumulative:  tensor([29.2174])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 2, 30)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_future_timesteps = 10\n",
    "iterative_predictions_tensor_np = iterative_path_predict(batch, dummy_net2, coordinate_system_enum, num_predict_steps=n_future_timesteps, bool_normalize_center = False)\n",
    "iterative_predictions_tensor_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10000000000000142"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33.7337 - 33.8337\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.22743 , 29.24743 , 29.27743 , 29.28743 , 29.30743 , 29.337431,\n",
       "       29.34743 , 29.36743 , 29.39743 , 29.40743 , 29.42743 , 29.45743 ,\n",
       "       29.46743 , 29.48743 , 29.51743 , 29.52743 , 29.54743 , 29.57743 ,\n",
       "       29.587431, 29.60743 , 29.63743 , 29.64743 , 29.66743 , 29.69743 ,\n",
       "       29.70743 , 29.72743 , 29.757431, 29.76743 , 29.78743 , 29.81743 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_predictions_tensor_np[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2, 30)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_predictions_tensor_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 29.1860,  29.1899,  29.1935,  29.1970,  29.2007,  29.2041,  29.2073,\n",
       "          29.2106,  29.2140,  29.2174],\n",
       "        [-90.5450, -90.5462, -90.5472, -90.5483, -90.5493, -90.5504, -90.5514,\n",
       "         -90.5523, -90.5534, -90.5544]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = batch['normalization_tensor'][0].clone()\n",
    "orig = input_tensor[0, :2, -10:] + n\n",
    "orig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 29.22743 ,  29.24743 ,  29.27743 ,  29.28743 ,  29.30743 ,\n",
       "         29.337431,  29.34743 ,  29.36743 ,  29.39743 ,  29.40743 ,\n",
       "         29.42743 ,  29.45743 ,  29.46743 ,  29.48743 ,  29.51743 ,\n",
       "         29.52743 ,  29.54743 ,  29.57743 ,  29.587431,  29.60743 ,\n",
       "         29.63743 ,  29.64743 ,  29.66743 ,  29.69743 ,  29.70743 ,\n",
       "         29.72743 ,  29.757431,  29.76743 ,  29.78743 ,  29.81743 ],\n",
       "       [-90.544426, -90.52443 , -90.49443 , -90.48443 , -90.46443 ,\n",
       "        -90.434425, -90.42443 , -90.40443 , -90.37443 , -90.364426,\n",
       "        -90.34443 , -90.31443 , -90.30443 , -90.28443 , -90.254425,\n",
       "        -90.24443 , -90.22443 , -90.19443 , -90.184425, -90.16443 ,\n",
       "        -90.13443 , -90.12443 , -90.10443 , -90.074425, -90.06443 ,\n",
       "        -90.044426, -90.01443 , -90.004425, -89.98443 , -89.95443 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(iterative_predictions_tensor_np.shape)\n",
    "iterative_predictions_tensor_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.010029999999996875"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29.2174 - 29.22743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10997999999999308"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-105.8839 + 105.77392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_7cb021be6df6e93a84cca606015e3f25 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_7cb021be6df6e93a84cca606015e3f25&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_7cb021be6df6e93a84cca606015e3f25 = L.map(\n",
       "                &quot;map_7cb021be6df6e93a84cca606015e3f25&quot;,\n",
       "                {\n",
       "                    center: [29.522430419921875, -90.24942779541016],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 4,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_4a3d8ae7d04f897651c48d666c2db9df = L.tileLayer(\n",
       "                &quot;https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors \\u0026copy; \\u003ca href=\\&quot;https://carto.com/attributions\\&quot;\\u003eCARTO\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 20, &quot;maxZoom&quot;: 20, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abcd&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_4a3d8ae7d04f897651c48d666c2db9df.addTo(map_7cb021be6df6e93a84cca606015e3f25);\n",
       "        \n",
       "    \n",
       "            map_7cb021be6df6e93a84cca606015e3f25.fitBounds(\n",
       "                [[29.10943031311035, -90.63292541503907], [29.9354305267334, -89.86593017578124]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            var poly_line_71d6e88cdbb913d98dedd27337b1f44e = L.polyline(\n",
       "                [[29.22743034362793, -90.54442596435547], [29.2474308013916, -90.52442932128906], [29.277429580688477, -90.49443054199219], [29.287429809570312, -90.48442840576172], [29.307430267333984, -90.46443176269531], [29.337430953979492, -90.4344253540039], [29.347429275512695, -90.42443084716797], [29.367429733276367, -90.40442657470703], [29.397430419921875, -90.37442779541016], [29.40743064880371, -90.36442565917969], [29.42742919921875, -90.34442901611328], [29.457429885864258, -90.3144302368164], [29.467430114746094, -90.30442810058594], [29.487430572509766, -90.28443145751953], [29.51742935180664, -90.25442504882812], [29.527429580688477, -90.24443054199219], [29.54743003845215, -90.22442626953125], [29.577430725097656, -90.19442749023438], [29.587430953979492, -90.1844253540039], [29.60742950439453, -90.1644287109375], [29.63743019104004, -90.13442993164062], [29.647430419921875, -90.12442779541016], [29.667430877685547, -90.10443115234375], [29.697429656982422, -90.07442474365234], [29.707429885864258, -90.0644302368164], [29.72743034362793, -90.04442596435547], [29.757431030273438, -90.0144271850586], [29.76742935180664, -90.00442504882812], [29.787429809570312, -89.98442840576172], [29.81743049621582, -89.95442962646484]],\n",
       "                {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;red&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: false, &quot;fillColor&quot;: &quot;red&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;noClip&quot;: false, &quot;opacity&quot;: 1, &quot;smoothFactor&quot;: 1.0, &quot;stroke&quot;: true, &quot;weight&quot;: 2.5}\n",
       "            ).addTo(map_7cb021be6df6e93a84cca606015e3f25);\n",
       "        \n",
       "    \n",
       "            var poly_line_5fc17c9c7cc1b434347a59b7b17725dc = L.polyline(\n",
       "                [[29.185989379882812, -90.54496765136719], [29.189910888671875, -90.54617309570312], [29.19348907470703, -90.5472183227539], [29.197019577026367, -90.54827117919922], [29.20067024230957, -90.54933166503906], [29.204059600830078, -90.55037689208984], [29.207319259643555, -90.55135345458984], [29.210649490356445, -90.55233001708984], [29.214040756225586, -90.55338287353516], [29.217430114746094, -90.55442810058594]],\n",
       "                {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;black&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: false, &quot;fillColor&quot;: &quot;black&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;noClip&quot;: false, &quot;opacity&quot;: 1, &quot;smoothFactor&quot;: 1.0, &quot;stroke&quot;: true, &quot;weight&quot;: 2.5}\n",
       "            ).addTo(map_7cb021be6df6e93a84cca606015e3f25);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x3377b5640>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat = iterative_predictions_tensor_np[0, 0]\n",
    "long = iterative_predictions_tensor_np[0, 1]\n",
    "min_lat, max_lat = np.min(lat), np.amax(lat)\n",
    "min_long, max_long = np.min(long), np.amax(long)\n",
    "m = create_folium_map(min_lat, min_long, max_lat, max_long, border_lat_prop=0.2, border_long_prop=0.15, tiles=None)\n",
    "\n",
    "folium.PolyLine(locations=iterative_predictions_tensor_np[0].T, color='red', weight=2.5, opacity=1).add_to(m)\n",
    "folium.PolyLine(locations=orig.T, color='black', weight=2.5, opacity=1).add_to(m)\n",
    "\n",
    "m\n",
    "\n",
    "# m = create_folium_map(min_lat, min_long, max_lat, max_long, border_lat_prop=0.2, border_long_prop=0.15, tiles=None) #\"Cartodb dark_matter\")\n",
    "# folium.PolyLine(locations=flightpath_compleate, color='black', weight=2.5, opacity=1).add_to(m)\n",
    "\n",
    "# for predicted_flightpath in predicted_paths_list:\n",
    "#     folium.PolyLine(locations=predicted_flightpath, color='blue', weight=2.5, opacity=1).add_to(m)\n",
    "\n",
    "\n",
    "\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def iterative_path_predict_step(input_tensor, prediction_tensor, coordinate_system_enum, len_coordinate_system):\n",
    "#     with torch.no_grad():\n",
    "#         _, _, num_input_rows = input_tensor.shape\n",
    "#         _, _, num_output_rows = prediction_tensor.shape\n",
    "#         input_tensor_final_n_rows = input_tensor[:, :, -num_output_rows:]  \n",
    "#         stacked_orig_pred = torch.concat([input_tensor, input_tensor_final_n_rows], dim=-1) # keep auxilary info, pred tensor will then replace the coordinate values\n",
    "#         stacked_orig_pred[:, :len_coordinate_system, -num_output_rows:] = prediction_tensor \n",
    "#         new_normalization_tensor = stacked_orig_pred[:, :len_coordinate_system, -1: ]\n",
    "\n",
    "#         print(\"inside step: \", stacked_orig_pred[0, 0, -10:])\n",
    "#         print(\"inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "#         stacked_orig_pred[:, :len_coordinate_system, :] = stacked_orig_pred[:, :len_coordinate_system, :] - new_normalization_tensor # new input tensor\n",
    "\n",
    "#         print(\"Before end of WITH inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "#     print(\"BEFORE RETURN inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "#     return torch.clone(stacked_orig_pred[:, :, -num_input_rows:]), torch.clone(new_normalization_tensor)\n",
    "\n",
    "def iterative_path_predict_step(input_tensor, prediction_tensor, coordinate_system_enum, len_coordinate_system):\n",
    "    with torch.no_grad():\n",
    "        _, _, num_input_rows = input_tensor.shape\n",
    "        _, _, num_output_rows = prediction_tensor.shape\n",
    "        input_tensor_final_n_rows = input_tensor[:, :, -num_output_rows:]  \n",
    "        stacked_orig_pred = torch.concat([input_tensor, input_tensor_final_n_rows], dim=-1) # keep auxiliary info, pred tensor will then replace the coordinate values\n",
    "        stacked_orig_pred[:, :len_coordinate_system, -num_output_rows:] = prediction_tensor \n",
    "        new_normalization_tensor = stacked_orig_pred[:, :len_coordinate_system, -1:].clone()  # Clone to avoid modifying the original tensor\n",
    "\n",
    "        #print(\"inside step: \", stacked_orig_pred[0, 0, -10:])\n",
    "        #print(\"inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "        stacked_orig_pred[:, :len_coordinate_system, :] = stacked_orig_pred[:, :len_coordinate_system, :] - new_normalization_tensor # new input tensor\n",
    "\n",
    "        #print(\"Before end of WITH inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "\n",
    "    #print(\"BEFORE RETURN inside step new_normalization_tensor: \", new_normalization_tensor[0, 0])\n",
    "    return torch.clone(stacked_orig_pred[:, :, -num_input_rows:]), torch.clone(new_normalization_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def iterative_path_predict(batch_dict, model, coordinate_system_enum, num_predict_steps, bool_normalize_center = False):\n",
    "    \"\"\"\n",
    "    iteratively predict N steps using the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coordinate_system = helper_get_coordinate_system_based_on_enum(coordinate_system_enum)\n",
    "    len_coordinate_system = len(coordinate_system )\n",
    "\n",
    "    if bool_normalize_center:\n",
    "        batch_dict = normalize_center(batch_dict, coordinate_system_enum=coordinate_system_enum)\n",
    "\n",
    "    input_tensor = batch_dict[\"input_tensor\"]\n",
    "    normalization_tensor_cumulative = torch.clone(batch_dict[\"normalization_tensor\"])\n",
    "    print(\"at the beginning normalization_tensor_cumulative: \", normalization_tensor_cumulative[0, 0])\n",
    "    prediction_tensor = model(input_tensor)\n",
    "    print(\"prediction_tensor   : \", prediction_tensor[0, 0, :10])\n",
    "\n",
    "    pred_tensors_list = [torch.clone(prediction_tensor)] \n",
    "\n",
    "    for i in range(num_predict_steps):\n",
    "        input_tensor, normalization_tensor = iterative_path_predict_step(input_tensor, prediction_tensor, coordinate_system_enum, len_coordinate_system)\n",
    "        prediction_tensor = model(input_tensor)\n",
    "        #print(\"prediction_tensor: \", prediction_tensor[0, 0, :10])\n",
    "        #print(\"normalization_tensor: \", normalization_tensor[0, 0])\n",
    "        #print(\"new input_tensor: \", input_tensor[0, 0, -10:])\n",
    "        normalization_tensor_cumulative = normalization_tensor_cumulative + normalization_tensor\n",
    "        pred_tensors_list.append(prediction_tensor)\n",
    "\n",
    "        print(\"normalization_tensor_cumulative: \", normalization_tensor_cumulative[0])\n",
    "\n",
    "    \n",
    "    iterative_predictions_tensor = torch.concat(pred_tensors_list, dim=-1) # with normalization: normalization_tensor_copy\n",
    "    print(\"at the end iterative_predictions_tensor: \", iterative_predictions_tensor[0, 0, -10:])\n",
    "    print(\"at the end normalization_tensor_cumulative: \", normalization_tensor_cumulative[0, 0])\n",
    "    # iterative_predictions_tensor = un_normalize_center_predictions(iterative_predictions_tensor, normalization_tensor_cumulative) \n",
    "    # iterative_predictions_tensor_np = iterative_predictions_tensor.detach().cpu().numpy()\n",
    "    # return iterative_predictions_tensor_np\n",
    "\n",
    "    return iterative_predictions_tensor, normalization_tensor_cumulative\n",
    "\n",
    "\n",
    "\n",
    "def dummy_net(dummy_arg):\n",
    "    return 0.01 * torch.ones((32, 2, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the beginning normalization_tensor_cumulative:  tensor([39.3563])\n",
      "prediction_tensor   :  tensor([0.0100])\n",
      "normalization_tensor_cumulative:  tensor([[  39.3663],\n",
      "        [-105.8739]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.3763],\n",
      "        [-105.8639]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.3863],\n",
      "        [-105.8539]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.3963],\n",
      "        [-105.8439]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.4063],\n",
      "        [-105.8339]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.4163],\n",
      "        [-105.8239]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.4263],\n",
      "        [-105.8139]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.4363],\n",
      "        [-105.8039]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.4463],\n",
      "        [-105.7939]])\n",
      "normalization_tensor_cumulative:  tensor([[  39.4563],\n",
      "        [-105.7839]])\n",
      "at the end iterative_predictions_tensor:  tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100])\n",
      "at the end normalization_tensor_cumulative:  tensor([39.4563])\n"
     ]
    }
   ],
   "source": [
    "n_future_timesteps = 10\n",
    "iterative_predictions_tensor, normalization_tensor_cumulative = iterative_path_predict(batch, dummy_net, coordinate_system_enum, num_predict_steps=n_future_timesteps, bool_normalize_center = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]],\n",
       "\n",
       "        [[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "          0.0100, 0.0100, 0.0100]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_predictions_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  39.4563],\n",
       "         [-105.7839]],\n",
       "\n",
       "        [[  35.6005],\n",
       "         [-110.6333]],\n",
       "\n",
       "        [[  43.9342],\n",
       "         [ -85.1787]],\n",
       "\n",
       "        [[  33.7318],\n",
       "         [ -84.2669]],\n",
       "\n",
       "        [[  33.4189],\n",
       "         [ -93.9903]],\n",
       "\n",
       "        [[  32.6742],\n",
       "         [-116.8587]],\n",
       "\n",
       "        [[  33.7202],\n",
       "         [-111.5278]],\n",
       "\n",
       "        [[  42.8627],\n",
       "         [ -87.3010]],\n",
       "\n",
       "        [[  42.0674],\n",
       "         [ -87.7974]],\n",
       "\n",
       "        [[  34.2349],\n",
       "         [-112.0497]],\n",
       "\n",
       "        [[  33.7495],\n",
       "         [ -84.3348]],\n",
       "\n",
       "        [[  39.7617],\n",
       "         [ -77.9656]],\n",
       "\n",
       "        [[  40.8877],\n",
       "         [-111.8768]],\n",
       "\n",
       "        [[  34.8683],\n",
       "         [-116.3792]],\n",
       "\n",
       "        [[  30.4466],\n",
       "         [ -95.6123]],\n",
       "\n",
       "        [[  39.0388],\n",
       "         [-104.3287]],\n",
       "\n",
       "        [[  40.8908],\n",
       "         [ -73.8944]],\n",
       "\n",
       "        [[  43.3235],\n",
       "         [ -87.9746]],\n",
       "\n",
       "        [[  33.7478],\n",
       "         [ -84.3414]],\n",
       "\n",
       "        [[  30.0928],\n",
       "         [ -90.1655]],\n",
       "\n",
       "        [[  35.7002],\n",
       "         [ -84.1626]],\n",
       "\n",
       "        [[  33.5313],\n",
       "         [-111.9465]],\n",
       "\n",
       "        [[  30.7990],\n",
       "         [ -86.2781]],\n",
       "\n",
       "        [[  40.2222],\n",
       "         [ -83.2080]],\n",
       "\n",
       "        [[  34.3717],\n",
       "         [-118.7337]],\n",
       "\n",
       "        [[  42.2861],\n",
       "         [ -92.2021]],\n",
       "\n",
       "        [[  37.7157],\n",
       "         [-122.2808]],\n",
       "\n",
       "        [[  43.9105],\n",
       "         [ -69.9398]],\n",
       "\n",
       "        [[  32.9431],\n",
       "         [-105.9110]],\n",
       "\n",
       "        [[  36.2621],\n",
       "         [ -86.7068]],\n",
       "\n",
       "        [[  37.1281],\n",
       "         [ -77.1191]],\n",
       "\n",
       "        [[  41.7336],\n",
       "         [ -72.0603]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization_tensor_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900,\n",
       "        0.1000, 0.1100])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(iterative_predictions_tensor, axis=-1)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 11])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_predictions_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  39.466293,   39.466293,   39.466293,   39.466293,   39.466293,\n",
       "          39.466293,   39.466293,   39.466293,   39.466293,   39.466293,\n",
       "          39.466293],\n",
       "       [-105.77392 , -105.77392 , -105.77392 , -105.77392 , -105.77392 ,\n",
       "        -105.77392 , -105.77392 , -105.77392 , -105.77392 , -105.77392 ,\n",
       "        -105.77392 ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_predictions_tensor_np[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(iterative_predictions_tensor_np[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-122.3798"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(iterative_predictions_tensor_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value closest to zero: 29.99376\n"
     ]
    }
   ],
   "source": [
    "# Find the value closest to zero\n",
    "closest_to_zero = iterative_predictions_tensor_np.flat[np.abs(iterative_predictions_tensor_np).argmin()]\n",
    "\n",
    "print(\"Value closest to zero:\", closest_to_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_predictions_tensor_np[0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "         0.0100],\n",
       "        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
       "         0.0100]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(2, 10)*0.01\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900,\n",
       "         0.1000],\n",
       "        [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900,\n",
       "         0.1000]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(ones, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajectory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
